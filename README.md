# Eva8_s9_transformers


### Vision Transformers (ViT)
The Vision Transformer (ViT) model was introduced in a research paper published as a conference paper at ICLR 2021 titled “An Image is Worth 16*16 Words: Transformers for Image Recognition at Scale”.
Read more at: https://viso.ai/deep-learning/vision-transformer-vit/


### Assignment
- Assignment S9 is just getting started with the basics of self-attention and related stuff in ViT.
- In brief, the assignment requies the following:
    Build the following network:

    1. That takes a CIFAR10 image (32x32x3)
    2. Add 3 Convolutions to arrive at AxAx48 dimensions (e.g. 32x32x3 | 3x3x3x16 >> 3x3x16x32 >> 3x3x32x48)
    3. Apply GAP and get 1x1x48, call this X
    4. Create a block called ULTIMUS that:
        1. Creates 3 FC layers called K, Q and V such that:
            1. X*K = 48*48x8 > 8
            2. X*Q = 48*48x8 > 8 
            3. X*V = 48*48x8 > 8 
        2. then create AM = SoftMax(QTK)/(8^0.5) = 8*8 = 8
        3. then Z = V*AM = 8*8 > 8
        4. then another FC layer called Out that:
            1. Z*Out = 8*8x48 > 48
    5. Repeat this Ultimus block 4 times
    6. Then add final FC layer that converts 48 to 10 and sends it to the loss function.
    7. Model would look like this C>C>C>U>U>U>U>FFC>Loss
    8. Train the model for 24 epochs using the OCP that I wrote in class. Use ADAM as an optimizer.


### Solution:
1. Please refer to the [models](/models/) module it contians the list of the models that could be used for various vision related tasks.
    1. Refer to [Assignment_9_Solution.ipynb](/Assignment_9_Solution.ipynb) for solution.
2. However, we would be using [model_9.py](/models/model_9.py) module for this attention task.
3. The model_9.py has the following network structure:

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 32, 32]             448
            Conv2d-2           [-1, 32, 32, 32]           4,640
            Conv2d-3           [-1, 48, 32, 32]          13,872
         AvgPool2d-4             [-1, 48, 1, 1]               0
            Linear-5                    [-1, 8]             392
            Linear-6                    [-1, 8]             392
            Linear-7                    [-1, 8]             392
            Linear-8                   [-1, 48]             432
      UltimusBlock-9                   [-1, 48]               0
           Linear-10                    [-1, 8]             392
           Linear-11                    [-1, 8]             392
           Linear-12                    [-1, 8]             392
           Linear-13                   [-1, 48]             432
     UltimusBlock-14                   [-1, 48]               0
           Linear-15                    [-1, 8]             392
           Linear-16                    [-1, 8]             392
           Linear-17                    [-1, 8]             392
           Linear-18                   [-1, 48]             432
     UltimusBlock-19                   [-1, 48]               0
           Linear-20                    [-1, 8]             392
           Linear-21                    [-1, 8]             392
           Linear-22                    [-1, 8]             392
           Linear-23                   [-1, 48]             432
     UltimusBlock-24                   [-1, 48]               0
           Linear-25                   [-1, 10]             490
================================================================
Total params: 25,882
Trainable params: 25,882
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 0.75
Params size (MB): 0.10
Estimated Total Size (MB): 0.86
----------------------------------------------------------------
```

4. [UltimusBlock](/models/model_9.py): This contains the linear layers responsible for the computation of Keys, Values and Queries required for the computation of attention matrix.

5. LRFinder has been used to find the optimum learning rate.
![alt text](/images/output.png)

6. Adam optimizer along with CrossEntropyLoss has been used for this task.
7. On setting the optimum LR (from LRFinder) as the ```max_lr```, and the ```one-tenth of max_lr``` as ```base_lr```, the model has been freshly trained and tested on trainload and testloader respectively for 24 epochs.

#### Training and testing logs
```
Epoch: 1
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.15it/s]
training loss: 2.3053, accuracy: 10.00, Optimzer LR: 0.00017174858176145227
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.00it/s]
Test loss: 2.3034, accuracy: 9.77
Epoch: 2
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.48it/s]
training loss: 2.3029, accuracy: 10.16, Optimzer LR: 0.00031674716352290456
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.27it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 3
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.60it/s]
training loss: 2.3028, accuracy: 9.60, Optimzer LR: 0.00046174574528435675
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.08it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 4
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.69it/s]
training loss: 2.3028, accuracy: 10.07, Optimzer LR: 0.000606744327045809
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.74it/s]
Test loss: 2.3028, accuracy: 9.77
Epoch: 5
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.65it/s]
training loss: 2.3028, accuracy: 9.71, Optimzer LR: 0.0007517429088072613
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.56it/s]
Test loss: 2.3027, accuracy: 10.21
Epoch: 6
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.50it/s]
training loss: 2.3028, accuracy: 9.88, Optimzer LR: 0.0008967414905687135
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.10it/s]
Test loss: 2.3027, accuracy: 10.21
Epoch: 7
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.12it/s]
training loss: 2.3028, accuracy: 9.85, Optimzer LR: 0.001041740072330166
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.65it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 8
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.81it/s]
training loss: 2.3027, accuracy: 9.95, Optimzer LR: 0.0010187227958318148
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.81it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 9
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 23.99it/s]
training loss: 2.3027, accuracy: 9.86, Optimzer LR: 0.0009550324788675292
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.97it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 10
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.65it/s]
training loss: 2.3027, accuracy: 9.73, Optimzer LR: 0.0008913421619032435
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.98it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 11
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.63it/s]
training loss: 2.3027, accuracy: 9.79, Optimzer LR: 0.0008276518449389578
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.03it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 12
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 23.52it/s]
training loss: 2.3026, accuracy: 9.68, Optimzer LR: 0.000763961527974672
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.06it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 13
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.27it/s]
training loss: 2.3026, accuracy: 10.06, Optimzer LR: 0.0007002712110103864
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.62it/s]
Test loss: 2.3026, accuracy: 9.91
Epoch: 14
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.08it/s]
training loss: 2.3026, accuracy: 9.65, Optimzer LR: 0.0006365808940461006
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.24it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 15
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.78it/s]
training loss: 2.3026, accuracy: 9.39, Optimzer LR: 0.0005728905770818149
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:02<00:00, 13.37it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 16
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.47it/s]
training loss: 2.3026, accuracy: 9.85, Optimzer LR: 0.0005092002601175292
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.06it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 17
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.49it/s]
training loss: 2.3026, accuracy: 9.59, Optimzer LR: 0.0004455099431532435
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.98it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 18
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 23.40it/s]
training loss: 2.3026, accuracy: 9.83, Optimzer LR: 0.0003818196261889576
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.10it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 19
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.60it/s]
training loss: 2.3026, accuracy: 10.17, Optimzer LR: 0.0003181293092246719
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.08it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 20
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.72it/s]
training loss: 2.3026, accuracy: 9.86, Optimzer LR: 0.00025443899226038633
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.18it/s]
Test loss: 2.3026, accuracy: 9.77
Epoch: 21
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.64it/s]
training loss: 2.3026, accuracy: 9.74, Optimzer LR: 0.00019074867529610053
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 13.07it/s]
Test loss: 2.3026, accuracy: 10.06
Epoch: 22
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:07<00:00, 24.51it/s]
training loss: 2.3026, accuracy: 10.12, Optimzer LR: 0.00012705835833181484
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.72it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 23
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 23.83it/s]
training loss: 2.3026, accuracy: 10.11, Optimzer LR: 6.336804136752904e-05
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.70it/s]
Test loss: 2.3026, accuracy: 10.21
Epoch: 24
100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:08<00:00, 24.06it/s]
training loss: 2.3026, accuracy: 9.72, Optimzer LR: -3.2227559675643627e-07
100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:03<00:00, 12.74it/s]Test loss: 2.3026, accuracy: 10.21
```

#### Plots of Loss and Accuracy
#### Loss
![alt text](/images/losses.png)

#### Accuracy
![alt text](/images/accs.png)